{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"MLT-Copy1.ipynb","provenance":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"spFu74SyChMW","colab_type":"text"},"source":["# Import Libraries"]},{"cell_type":"code","metadata":{"id":"Bd6qrxFBChMX","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.layers import Input, LSTM, Dense\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import pandas as pd\n","import os\n","import io\n","import time\n","\n","import string\n","from string import digits"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zNSKt_PChMd","colab_type":"text"},"source":["# Load Data\n","Download English Arabic dataset. The data is available here: Data source: http://www.manythings.org/anki/"]},{"cell_type":"code","metadata":{"id":"4MW_5O0aDJbf","colab_type":"code","outputId":"d2ece8f9-dba1-4998-a497-a78e5b6fc813","executionInfo":{"status":"ok","timestamp":1585679364243,"user_tz":240,"elapsed":28204,"user":{"displayName":"Ahmad Barakat","photoUrl":"","userId":"14669862239562823268"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PNQI5amcChMd","colab_type":"code","outputId":"233fe185-f077-47d4-de8f-7336d2223975","executionInfo":{"status":"ok","timestamp":1585653695097,"user_tz":240,"elapsed":2918,"user":{"displayName":"Ahmad Barakat","photoUrl":"","userId":"14669862239562823268"}},"colab":{"base_uri":"https://localhost:8080/","height":196}},"source":["# Load data from repository \n","url = '/content/gdrive/My Drive/Colab Notebooks/Machine Translation English to Arabic /ara.txt'\n","lines = pd.read_csv(url,delimiter=\"\\t\",names=['input_texts', 'target_texts', 'notes'])\n","# Return the DataFrame head\n","lines.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input_texts</th>\n","      <th>target_texts</th>\n","      <th>notes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hi.</td>\n","      <td>مرحبًا.</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Run!</td>\n","      <td>اركض!</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Help!</td>\n","      <td>النجدة!</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Jump!</td>\n","      <td>اقفز!</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Stop!</td>\n","      <td>قف!</td>\n","      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  input_texts target_texts                                              notes\n","0         Hi.      مرحبًا.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n","1        Run!        اركض!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n","2       Help!      النجدة!  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n","3       Jump!        اقفز!  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n","4       Stop!          قف!  CC-BY 2.0 (France) Attribution: tatoeba.org #4..."]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"ejysqvevChMi","colab_type":"text"},"source":["# Explantory Data Analysis"]},{"cell_type":"code","metadata":{"id":"pOmsGZTXChMj","colab_type":"code","outputId":"f0c06454-bb54-4ee8-946b-c7f91f1e1993","executionInfo":{"status":"ok","timestamp":1585653698285,"user_tz":240,"elapsed":616,"user":{"displayName":"Ahmad Barakat","photoUrl":"","userId":"14669862239562823268"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# Return the dimensionality of the DataFrame.\n","lines.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11320, 3)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"fdct-4jMChMm","colab_type":"code","outputId":"feb892a2-bf22-4d39-c5fe-9dd05dc4e096","executionInfo":{"status":"ok","timestamp":1585653701574,"user_tz":240,"elapsed":562,"user":{"displayName":"Ahmad Barakat","photoUrl":"","userId":"14669862239562823268"}},"colab":{"base_uri":"https://localhost:8080/","height":150}},"source":["# Return information about the DataFrame including the index dtype and column dtypes, non-null values and memory usage.\n","lines.info()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 11320 entries, 0 to 11319\n","Data columns (total 3 columns):\n","input_texts     11320 non-null object\n","target_texts    11320 non-null object\n","notes           11320 non-null object\n","dtypes: object(3)\n","memory usage: 265.4+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IbcMLzPaChMq","colab_type":"text"},"source":["# Data Pre-Processing\n","This section is to perform some data cleaning and preparation."]},{"cell_type":"code","metadata":{"id":"vixATjgeChMr","colab_type":"code","colab":{}},"source":["# Remove a column by specifying label names \n","lines = lines.drop(columns='notes')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYI9MrX-ChMu","colab_type":"code","colab":{}},"source":["# Lowercase all characters\n","lines.input_texts=lines.input_texts.apply(lambda x: x.lower())\n","lines.target_texts=lines.target_texts.apply(lambda x: x.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8N6ECBU-ChMx","colab_type":"code","colab":{}},"source":["# Remove quotes\n","lines.input_texts=lines.input_texts.apply(lambda x: re.sub(\"'\", '', x))\n","lines.target_texts=lines.target_texts.apply(lambda x: re.sub(\"'\", '', x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"STS39ePcChM1","colab_type":"code","colab":{}},"source":["# Set of all special characters\n","english_punctuations = set(string.punctuation)\n","arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n","# Remove all the special characters\n","lines.input_texts=lines.input_texts.apply(lambda x: ''.join(ch for ch in x if ch not in english_punctuations))\n","lines.target_texts=lines.target_texts.apply(lambda x: ''.join(ch for ch in x if ch not in arabic_punctuations))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJSUIkoeChM4","colab_type":"code","colab":{}},"source":["# Remove Arabic diacritics from text\n","lines.target_texts=lines.target_texts.apply(lambda x: re.sub(\"[ ّ َ ً ُ ٌ ِ ٍ ْ]\", \" \", x))\n","\n","#  ّ    | # Tashdid\n","#  َ    | # Fatha\n","#  ً    | # Tanwin Fath\n","#  ُ    | # Damma\n","#  ٌ    | # Tanwin Damm\n","#  ِ    | # Kasra\n","#  ٍ    | # Tanwin Kasr\n","#  ْ    | # Sukun"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cS15BArWChM7","colab_type":"code","colab":{}},"source":["# Remove all numbers from text\n","remove_digits = str.maketrans('', '', digits)\n","lines.input_texts=lines.input_texts.apply(lambda x: x.translate(remove_digits))\n","lines.target_texts=lines.target_texts.apply(lambda x: re.sub(\"[١٢٣٤٥٦٧٨٩٠]\", \"\", x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NTuNUepChM-","colab_type":"code","colab":{}},"source":["# Remove extra spaces\n","lines.input_texts=lines.input_texts.apply(lambda x: x.strip())\n","lines.target_texts=lines.target_texts.apply(lambda x: x.strip())\n","lines.input_texts=lines.input_texts.apply(lambda x: re.sub(\" +\", \" \", x))\n","lines.target_texts=lines.target_texts.apply(lambda x: re.sub(\" +\", \" \", x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOvL0PvRChNG","colab_type":"code","colab":{}},"source":["lines.to_csv(r'/content/gdrive/My Drive/Colab Notebooks/Machine Translation English to Arabic /arabic.txt', header=None, index=None, sep='\\t', mode='a')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yXkS4cHChNI","colab_type":"text"},"source":["# Text Data Vectorization\n","This section is to victorize the data and compute the vocabulary for both English and Arabic. Main steps:\n","- compute the vocabulary sizes,\n","- compute the length of maximum sequence for both languages,\n","- create dictionaries to convert a given token into an integer index and vice-versa."]},{"cell_type":"code","metadata":{"id":"tQbwyqtzChNY","colab_type":"code","colab":{}},"source":["batch_size = 64  # Batch size for training.\n","epochs = 100 #umber of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 11000  # Number of samples to train on."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhyLotntChNJ","colab_type":"code","outputId":"d5a0930b-2e2f-4bcb-b1ba-2508269a81df","executionInfo":{"status":"ok","timestamp":1585679802904,"user_tz":240,"elapsed":898,"user":{"displayName":"Ahmad Barakat","photoUrl":"","userId":"14669862239562823268"}},"colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["# Vectorize the data.\n","data_path = '/content/gdrive/My Drive/Colab Notebooks/Machine Translation English to Arabic /arabic.txt'\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text = line.split('\\t')\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print('Number of samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Number of samples: 11000\n","Number of unique input tokens: 28\n","Number of unique output tokens: 79\n","Max sequence length for inputs: 55\n","Max sequence length for outputs: 67\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dCj2QKksChNM","colab_type":"code","colab":{}},"source":["input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyDcIgKrChNS","colab_type":"code","colab":{}},"source":["for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n","    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpY61X-TChNV","colab_type":"code","colab":{}},"source":["# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FD7--c1lChNX","colab_type":"text"},"source":["# Model Development"]},{"cell_type":"code","metadata":{"id":"6aYhHwgJChNb","colab_type":"code","colab":{}},"source":["# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWJPwAm7ChNe","colab_type":"code","outputId":"b6ef35a4-c249-4bfb-b5ab-3ee27ea4596e","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1585680083496,"user_tz":240,"elapsed":265234,"user":{"displayName":"Ahmad Barakat","photoUrl":"","userId":"14669862239562823268"}}},"source":["# Run training\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.3,shuffle=True)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","121/121 [==============================] - 3s 28ms/step - loss: 1.0124 - accuracy: 0.7724 - val_loss: 1.7022 - val_accuracy: 0.6315\n","Epoch 2/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.8075 - accuracy: 0.7863 - val_loss: 1.3115 - val_accuracy: 0.6372\n","Epoch 3/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.7271 - accuracy: 0.8029 - val_loss: 1.2070 - val_accuracy: 0.6767\n","Epoch 4/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.6720 - accuracy: 0.8164 - val_loss: 1.1658 - val_accuracy: 0.6761\n","Epoch 5/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.6484 - accuracy: 0.8214 - val_loss: 1.1435 - val_accuracy: 0.6836\n","Epoch 6/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.6289 - accuracy: 0.8251 - val_loss: 1.0905 - val_accuracy: 0.6993\n","Epoch 7/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.6115 - accuracy: 0.8292 - val_loss: 1.0764 - val_accuracy: 0.7015\n","Epoch 8/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.5933 - accuracy: 0.8331 - val_loss: 1.0678 - val_accuracy: 0.7029\n","Epoch 9/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.5827 - accuracy: 0.8372 - val_loss: 1.0538 - val_accuracy: 0.7036\n","Epoch 10/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.5657 - accuracy: 0.8398 - val_loss: 1.0361 - val_accuracy: 0.7086\n","Epoch 11/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.5527 - accuracy: 0.8435 - val_loss: 1.0178 - val_accuracy: 0.7120\n","Epoch 12/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.5403 - accuracy: 0.8467 - val_loss: 1.0289 - val_accuracy: 0.7092\n","Epoch 13/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.5291 - accuracy: 0.8498 - val_loss: 1.0105 - val_accuracy: 0.7162\n","Epoch 14/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.5184 - accuracy: 0.8529 - val_loss: 0.9972 - val_accuracy: 0.7185\n","Epoch 15/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.5075 - accuracy: 0.8557 - val_loss: 0.9888 - val_accuracy: 0.7226\n","Epoch 16/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.4973 - accuracy: 0.8586 - val_loss: 0.9876 - val_accuracy: 0.7236\n","Epoch 17/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.4879 - accuracy: 0.8611 - val_loss: 0.9785 - val_accuracy: 0.7253\n","Epoch 18/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4778 - accuracy: 0.8640 - val_loss: 0.9684 - val_accuracy: 0.7298\n","Epoch 19/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4676 - accuracy: 0.8667 - val_loss: 0.9833 - val_accuracy: 0.7275\n","Epoch 20/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4579 - accuracy: 0.8696 - val_loss: 0.9775 - val_accuracy: 0.7275\n","Epoch 21/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4487 - accuracy: 0.8721 - val_loss: 0.9808 - val_accuracy: 0.7294\n","Epoch 22/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4386 - accuracy: 0.8750 - val_loss: 0.9630 - val_accuracy: 0.7352\n","Epoch 23/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4292 - accuracy: 0.8775 - val_loss: 0.9854 - val_accuracy: 0.7303\n","Epoch 24/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4204 - accuracy: 0.8805 - val_loss: 0.9712 - val_accuracy: 0.7371\n","Epoch 25/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4113 - accuracy: 0.8831 - val_loss: 0.9855 - val_accuracy: 0.7352\n","Epoch 26/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.4033 - accuracy: 0.8855 - val_loss: 0.9937 - val_accuracy: 0.7348\n","Epoch 27/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3949 - accuracy: 0.8876 - val_loss: 0.9897 - val_accuracy: 0.7359\n","Epoch 28/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3866 - accuracy: 0.8902 - val_loss: 1.0015 - val_accuracy: 0.7347\n","Epoch 29/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3790 - accuracy: 0.8924 - val_loss: 1.0225 - val_accuracy: 0.7303\n","Epoch 30/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3710 - accuracy: 0.8946 - val_loss: 1.0307 - val_accuracy: 0.7315\n","Epoch 31/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3637 - accuracy: 0.8966 - val_loss: 1.0311 - val_accuracy: 0.7336\n","Epoch 32/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3565 - accuracy: 0.8991 - val_loss: 1.0329 - val_accuracy: 0.7351\n","Epoch 33/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3492 - accuracy: 0.9014 - val_loss: 1.0520 - val_accuracy: 0.7342\n","Epoch 34/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3424 - accuracy: 0.9031 - val_loss: 1.0491 - val_accuracy: 0.7333\n","Epoch 35/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3354 - accuracy: 0.9054 - val_loss: 1.0680 - val_accuracy: 0.7337\n","Epoch 36/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3300 - accuracy: 0.9067 - val_loss: 1.0479 - val_accuracy: 0.7375\n","Epoch 37/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3238 - accuracy: 0.9086 - val_loss: 1.0726 - val_accuracy: 0.7332\n","Epoch 38/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3175 - accuracy: 0.9107 - val_loss: 1.0839 - val_accuracy: 0.7343\n","Epoch 39/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3118 - accuracy: 0.9123 - val_loss: 1.1092 - val_accuracy: 0.7317\n","Epoch 40/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.3056 - accuracy: 0.9141 - val_loss: 1.1000 - val_accuracy: 0.7351\n","Epoch 41/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2993 - accuracy: 0.9158 - val_loss: 1.1281 - val_accuracy: 0.7314\n","Epoch 42/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2953 - accuracy: 0.9170 - val_loss: 1.1177 - val_accuracy: 0.7333\n","Epoch 43/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2889 - accuracy: 0.9186 - val_loss: 1.1333 - val_accuracy: 0.7344\n","Epoch 44/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2837 - accuracy: 0.9205 - val_loss: 1.1633 - val_accuracy: 0.7316\n","Epoch 45/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2785 - accuracy: 0.9221 - val_loss: 1.1574 - val_accuracy: 0.7337\n","Epoch 46/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2769 - accuracy: 0.9226 - val_loss: 1.1744 - val_accuracy: 0.7312\n","Epoch 47/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.2693 - accuracy: 0.9247 - val_loss: 1.2038 - val_accuracy: 0.7302\n","Epoch 48/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.2672 - accuracy: 0.9253 - val_loss: 1.2241 - val_accuracy: 0.7263\n","Epoch 49/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2614 - accuracy: 0.9269 - val_loss: 1.2144 - val_accuracy: 0.7309\n","Epoch 50/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2568 - accuracy: 0.9285 - val_loss: 1.2160 - val_accuracy: 0.7300\n","Epoch 51/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2525 - accuracy: 0.9295 - val_loss: 1.2504 - val_accuracy: 0.7281\n","Epoch 52/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2493 - accuracy: 0.9307 - val_loss: 1.2594 - val_accuracy: 0.7283\n","Epoch 53/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2458 - accuracy: 0.9315 - val_loss: 1.2903 - val_accuracy: 0.7263\n","Epoch 54/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.2414 - accuracy: 0.9330 - val_loss: 1.2856 - val_accuracy: 0.7277\n","Epoch 55/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2372 - accuracy: 0.9339 - val_loss: 1.2950 - val_accuracy: 0.7287\n","Epoch 56/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2337 - accuracy: 0.9349 - val_loss: 1.2995 - val_accuracy: 0.7288\n","Epoch 57/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2305 - accuracy: 0.9359 - val_loss: 1.3328 - val_accuracy: 0.7258\n","Epoch 58/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2263 - accuracy: 0.9372 - val_loss: 1.3347 - val_accuracy: 0.7280\n","Epoch 59/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2234 - accuracy: 0.9380 - val_loss: 1.3480 - val_accuracy: 0.7266\n","Epoch 60/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2203 - accuracy: 0.9391 - val_loss: 1.3468 - val_accuracy: 0.7280\n","Epoch 61/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2171 - accuracy: 0.9400 - val_loss: 1.3624 - val_accuracy: 0.7271\n","Epoch 62/100\n","121/121 [==============================] - 3s 22ms/step - loss: 0.2146 - accuracy: 0.9407 - val_loss: 1.3792 - val_accuracy: 0.7268\n","Epoch 63/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2114 - accuracy: 0.9416 - val_loss: 1.3828 - val_accuracy: 0.7259\n","Epoch 64/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2085 - accuracy: 0.9424 - val_loss: 1.3981 - val_accuracy: 0.7270\n","Epoch 65/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2058 - accuracy: 0.9431 - val_loss: 1.3989 - val_accuracy: 0.7268\n","Epoch 66/100\n","121/121 [==============================] - 3s 23ms/step - loss: 0.2034 - accuracy: 0.9439 - val_loss: 1.4141 - val_accuracy: 0.7271\n","Epoch 67/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.2004 - accuracy: 0.9448 - val_loss: 1.4125 - val_accuracy: 0.7282\n","Epoch 68/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1980 - accuracy: 0.9454 - val_loss: 1.4268 - val_accuracy: 0.7269\n","Epoch 69/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1954 - accuracy: 0.9463 - val_loss: 1.4381 - val_accuracy: 0.7289\n","Epoch 70/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1930 - accuracy: 0.9468 - val_loss: 1.4576 - val_accuracy: 0.7266\n","Epoch 71/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1910 - accuracy: 0.9473 - val_loss: 1.4646 - val_accuracy: 0.7267\n","Epoch 72/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1886 - accuracy: 0.9480 - val_loss: 1.4703 - val_accuracy: 0.7270\n","Epoch 73/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1859 - accuracy: 0.9485 - val_loss: 1.5049 - val_accuracy: 0.7246\n","Epoch 74/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1838 - accuracy: 0.9495 - val_loss: 1.4770 - val_accuracy: 0.7280\n","Epoch 75/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1821 - accuracy: 0.9497 - val_loss: 1.5121 - val_accuracy: 0.7250\n","Epoch 76/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1797 - accuracy: 0.9503 - val_loss: 1.5056 - val_accuracy: 0.7266\n","Epoch 77/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1779 - accuracy: 0.9507 - val_loss: 1.5259 - val_accuracy: 0.7260\n","Epoch 78/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1762 - accuracy: 0.9512 - val_loss: 1.5352 - val_accuracy: 0.7267\n","Epoch 79/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1763 - accuracy: 0.9516 - val_loss: 1.5363 - val_accuracy: 0.7254\n","Epoch 80/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1734 - accuracy: 0.9521 - val_loss: 1.5467 - val_accuracy: 0.7267\n","Epoch 81/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1715 - accuracy: 0.9526 - val_loss: 1.5567 - val_accuracy: 0.7270\n","Epoch 82/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1702 - accuracy: 0.9532 - val_loss: 1.5606 - val_accuracy: 0.7274\n","Epoch 83/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1677 - accuracy: 0.9536 - val_loss: 1.5731 - val_accuracy: 0.7261\n","Epoch 84/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1665 - accuracy: 0.9540 - val_loss: 1.5892 - val_accuracy: 0.7260\n","Epoch 85/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1647 - accuracy: 0.9542 - val_loss: 1.5975 - val_accuracy: 0.7252\n","Epoch 86/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1633 - accuracy: 0.9545 - val_loss: 1.5975 - val_accuracy: 0.7265\n","Epoch 87/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1611 - accuracy: 0.9556 - val_loss: 1.6137 - val_accuracy: 0.7251\n","Epoch 88/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1600 - accuracy: 0.9555 - val_loss: 1.6261 - val_accuracy: 0.7255\n","Epoch 89/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1588 - accuracy: 0.9557 - val_loss: 1.6372 - val_accuracy: 0.7247\n","Epoch 90/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1568 - accuracy: 0.9564 - val_loss: 1.6435 - val_accuracy: 0.7251\n","Epoch 91/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1554 - accuracy: 0.9568 - val_loss: 1.6423 - val_accuracy: 0.7250\n","Epoch 92/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1537 - accuracy: 0.9572 - val_loss: 1.6455 - val_accuracy: 0.7264\n","Epoch 93/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1524 - accuracy: 0.9577 - val_loss: 1.6485 - val_accuracy: 0.7263\n","Epoch 94/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1512 - accuracy: 0.9579 - val_loss: 1.6751 - val_accuracy: 0.7243\n","Epoch 95/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1499 - accuracy: 0.9583 - val_loss: 1.6923 - val_accuracy: 0.7248\n","Epoch 96/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1490 - accuracy: 0.9583 - val_loss: 1.6654 - val_accuracy: 0.7265\n","Epoch 97/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1476 - accuracy: 0.9590 - val_loss: 1.6901 - val_accuracy: 0.7259\n","Epoch 98/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1462 - accuracy: 0.9591 - val_loss: 1.6970 - val_accuracy: 0.7260\n","Epoch 99/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1449 - accuracy: 0.9595 - val_loss: 1.7159 - val_accuracy: 0.7245\n","Epoch 100/100\n","121/121 [==============================] - 3s 21ms/step - loss: 0.1438 - accuracy: 0.9599 - val_loss: 1.7169 - val_accuracy: 0.7257\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f5da251d0f0>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"eiKU1B6NChNh","colab_type":"code","colab":{}},"source":["# Save model\n","model.save('/content/gdrive/My Drive/Colab Notebooks/Machine Translation English to Arabic /s2s_ver6.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7W8sKsIChNj","colab_type":"code","colab":{}},"source":["# Next: inference mode (sampling).\n","# Here's the drill:\n","# 1) encode input and retrieve initial decoder state\n","# 2) run one step of decoder with this initial state\n","# and a \"start of sequence\" token as target.\n","# Output will be the next target token\n","# 3) Repeat with the current target token and current states\n","\n","# Define sampling models\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNH4bsG2ChNl","colab_type":"code","colab":{}},"source":["# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"giZPiuzUChNn","colab_type":"code","colab":{}},"source":["def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4nLz31SChNq","colab_type":"code","outputId":"2459dfb8-aa35-492b-92ab-be6670340b34","executionInfo":{"status":"ok","timestamp":1585680175647,"user_tz":240,"elapsed":92072,"user":{"displayName":"Ahmad Barakat","photoUrl":"","userId":"14669862239562823268"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for seq_index in range(200):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["-\n","Input sentence: hi\n","Decoded sentence: الطاب باليوبور\n","\n","-\n","Input sentence: run\n","Decoded sentence: هل أنت محق\n","\n","-\n","Input sentence: help\n","Decoded sentence: هل حادة متعدما\n","\n","-\n","Input sentence: jump\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: stop\n","Decoded sentence: توقفي عن الصراخ\n","\n","-\n","Input sentence: go on\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: go on\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: hello\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: hurry\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: hurry\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: i see\n","Decoded sentence: استقلت\n","\n","-\n","Input sentence: i won\n","Decoded sentence: استيقظت للتو\n","\n","-\n","Input sentence: relax\n","Decoded sentence: استرح\n","\n","-\n","Input sentence: smile\n","Decoded sentence: انتبه\n","\n","-\n","Input sentence: cheers\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: got it\n","Decoded sentence: هل حدث\n","\n","-\n","Input sentence: he ran\n","Decoded sentence: هو تركني أذهب\n","\n","-\n","Input sentence: i know\n","Decoded sentence: أنا أحدده\n","\n","-\n","Input sentence: i know\n","Decoded sentence: أنا أحدده\n","\n","-\n","Input sentence: i know\n","Decoded sentence: أنا أحدده\n","\n","-\n","Input sentence: im\n","Decoded sentence: أنا ولد\n","\n","-\n","Input sentence: im ok\n","Decoded sentence: أنا لست سعيدا\n","\n","-\n","Input sentence: listen\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: no way\n","Decoded sentence: لا أحد يعلم\n","\n","-\n","Input sentence: really\n","Decoded sentence: استرح\n","\n","-\n","Input sentence: thanks\n","Decoded sentence: شكرا جزيلا\n","\n","-\n","Input sentence: why me\n","Decoded sentence: لماذا لا أن مشغول\n","\n","-\n","Input sentence: awesome\n","Decoded sentence: اعتني بكفا\n","\n","-\n","Input sentence: be cool\n","Decoded sentence: ك ن محددا\n","\n","-\n","Input sentence: call me\n","Decoded sentence: استطنع الحلب\n","\n","-\n","Input sentence: call me\n","Decoded sentence: استطنع الحلب\n","\n","-\n","Input sentence: come in\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: come in\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: come on\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: come on\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: come on\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: get out\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: get out\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: get out\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: go away\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: go away\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: go away\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: goodbye\n","Decoded sentence: ليلة سعيدة\n","\n","-\n","Input sentence: he came\n","Decoded sentence: إنه يبيع\n","\n","-\n","Input sentence: he runs\n","Decoded sentence: هو تركني أذهب\n","\n","-\n","Input sentence: help me\n","Decoded sentence: هل تحب ذلك\n","\n","-\n","Input sentence: help me\n","Decoded sentence: هل تحب ذلك\n","\n","-\n","Input sentence: im sad\n","Decoded sentence: أنا ولد\n","\n","-\n","Input sentence: me too\n","Decoded sentence: ك ن محددا\n","\n","-\n","Input sentence: shut up\n","Decoded sentence: إنها تتكلم كثيرا\n","\n","-\n","Input sentence: shut up\n","Decoded sentence: إنها تتكلم كثيرا\n","\n","-\n","Input sentence: shut up\n","Decoded sentence: إنها تتكلم كثيرا\n","\n","-\n","Input sentence: shut up\n","Decoded sentence: إنها تتكلم كثيرا\n","\n","-\n","Input sentence: stop it\n","Decoded sentence: توقفي عن الصراخ\n","\n","-\n","Input sentence: take it\n","Decoded sentence: الحس بالدرن\n","\n","-\n","Input sentence: tell me\n","Decoded sentence: أنا أحبك\n","\n","-\n","Input sentence: tom won\n","Decoded sentence: لقد تحسنت\n","\n","-\n","Input sentence: tom won\n","Decoded sentence: لقد تحسنت\n","\n","-\n","Input sentence: wake up\n","Decoded sentence: استمر\n","\n","-\n","Input sentence: welcome\n","Decoded sentence: الحر قهاد\n","\n","-\n","Input sentence: welcome\n","Decoded sentence: الحر قهاد\n","\n","-\n","Input sentence: welcome\n","Decoded sentence: الحر قهاد\n","\n","-\n","Input sentence: welcome\n","Decoded sentence: الحر قهاد\n","\n","-\n","Input sentence: who won\n","Decoded sentence: من يملك هذا المنزل\n","\n","-\n","Input sentence: who won\n","Decoded sentence: من يملك هذا المنزل\n","\n","-\n","Input sentence: why not\n","Decoded sentence: لماذا لا أن مشغول\n","\n","-\n","Input sentence: why not\n","Decoded sentence: لماذا لا أن مشغول\n","\n","-\n","Input sentence: have fun\n","Decoded sentence: استه إلي\n","\n","-\n","Input sentence: hurry up\n","Decoded sentence: ربما يوم\n","\n","-\n","Input sentence: i forgot\n","Decoded sentence: أنا أريد التحم\n","\n","-\n","Input sentence: i got it\n","Decoded sentence: أنا أريد التحم\n","\n","-\n","Input sentence: i got it\n","Decoded sentence: أنا أريد التحم\n","\n","-\n","Input sentence: i got it\n","Decoded sentence: أنا أريد التحم\n","\n","-\n","Input sentence: i use it\n","Decoded sentence: أنا أريد التحم\n","\n","-\n","Input sentence: ill pay\n","Decoded sentence: سأعود\n","\n","-\n","Input sentence: im busy\n","Decoded sentence: أنا سعيد جدا\n","\n","-\n","Input sentence: im busy\n","Decoded sentence: أنا سعيد جدا\n","\n","-\n","Input sentence: im cold\n","Decoded sentence: أنا سعيد للغاية\n","\n","-\n","Input sentence: im free\n","Decoded sentence: أنا أحبك\n","\n","-\n","Input sentence: im here\n","Decoded sentence: أنا أ ريد تنا\n","\n","-\n","Input sentence: im home\n","Decoded sentence: أنا سعيد للغاية\n","\n","-\n","Input sentence: im poor\n","Decoded sentence: أنا سعيد جدا\n","\n","-\n","Input sentence: im rich\n","Decoded sentence: أنا سعيد\n","\n","-\n","Input sentence: it hurts\n","Decoded sentence: كان الجو حارا\n","\n","-\n","Input sentence: its hot\n","Decoded sentence: إنه جديد\n","\n","-\n","Input sentence: its new\n","Decoded sentence: إنه جديد\n","\n","-\n","Input sentence: lets go\n","Decoded sentence: لنذهب\n","\n","-\n","Input sentence: lets go\n","Decoded sentence: لنذهب\n","\n","-\n","Input sentence: lets go\n","Decoded sentence: لنذهب\n","\n","-\n","Input sentence: lets go\n","Decoded sentence: لنذهب\n","\n","-\n","Input sentence: lets go\n","Decoded sentence: لنذهب\n","\n","-\n","Input sentence: look out\n","Decoded sentence: انظر إلي\n","\n","-\n","Input sentence: look out\n","Decoded sentence: انظر إلي\n","\n","-\n","Input sentence: look out\n","Decoded sentence: انظر إلي\n","\n","-\n","Input sentence: speak up\n","Decoded sentence: انتبه\n","\n","-\n","Input sentence: stand up\n","Decoded sentence: ها هو الباص\n","\n","-\n","Input sentence: terrific\n","Decoded sentence: الجو حار جدا\n","\n","-\n","Input sentence: terrific\n","Decoded sentence: الجو حار جدا\n","\n","-\n","Input sentence: tom died\n","Decoded sentence: توم هنا\n","\n","-\n","Input sentence: tom died\n","Decoded sentence: توم هنا\n","\n","-\n","Input sentence: tom left\n","Decoded sentence: لقد تحسنت هذا\n","\n","-\n","Input sentence: tom lied\n","Decoded sentence: توم يحبه\n","\n","-\n","Input sentence: tom lost\n","Decoded sentence: لقد تحسنت هذا\n","\n","-\n","Input sentence: tom quit\n","Decoded sentence: توم هنا\n","\n","-\n","Input sentence: try some\n","Decoded sentence: إنها تحب توم\n","\n","-\n","Input sentence: who am i\n","Decoded sentence: من يملك هذا المنزل\n","\n","-\n","Input sentence: who died\n","Decoded sentence: من يدر سك الفرجة\n","\n","-\n","Input sentence: who died\n","Decoded sentence: من يدر سك الفرجة\n","\n","-\n","Input sentence: after you\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: birds fly\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: birds fly\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: bless you\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: calm down\n","Decoded sentence: استهان توم\n","\n","-\n","Input sentence: can we go\n","Decoded sentence: اتصل بي\n","\n","-\n","Input sentence: come here\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: come here\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: come home\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: did i win\n","Decoded sentence: هل أنت محق\n","\n","-\n","Input sentence: do it now\n","Decoded sentence: افعل ما تريد\n","\n","-\n","Input sentence: dont lie\n","Decoded sentence: لا تلعب هنا\n","\n","-\n","Input sentence: excuse me\n","Decoded sentence: إنها تتحلم بسرعة\n","\n","-\n","Input sentence: fantastic\n","Decoded sentence: تعال إلى المال\n","\n","-\n","Input sentence: forget it\n","Decoded sentence: انس ذلك\n","\n","-\n","Input sentence: forget it\n","Decoded sentence: انس ذلك\n","\n","-\n","Input sentence: forget it\n","Decoded sentence: انس ذلك\n","\n","-\n","Input sentence: forget it\n","Decoded sentence: انس ذلك\n","\n","-\n","Input sentence: forget me\n","Decoded sentence: انس ذلك\n","\n","-\n","Input sentence: go inside\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: go to bed\n","Decoded sentence: استمع إلي\n","\n","-\n","Input sentence: good luck\n","Decoded sentence: ليلة سعيدة\n","\n","-\n","Input sentence: goodnight\n","Decoded sentence: سي حسا ه\n","\n","-\n","Input sentence: goodnight\n","Decoded sentence: سي حسا ه\n","\n","-\n","Input sentence: hands off\n","Decoded sentence: هذه الكلبير لت\n","\n","-\n","Input sentence: he is ill\n","Decoded sentence: إنه وحيد\n","\n","-\n","Input sentence: he is ill\n","Decoded sentence: إنه وحيد\n","\n","-\n","Input sentence: hows tom\n","Decoded sentence: كيف حال المدرسي\n","\n","-\n","Input sentence: hows tom\n","Decoded sentence: كيف حال المدرسي\n","\n","-\n","Input sentence: i am thai\n","Decoded sentence: أنا أحد ملذ ب\n","\n","-\n","Input sentence: i am cold\n","Decoded sentence: أنا أحودك\n","\n","-\n","Input sentence: i am okay\n","Decoded sentence: أنا أحودك\n","\n","-\n","Input sentence: i am sure\n","Decoded sentence: أنا أحودك\n","\n","-\n","Input sentence: i hope so\n","Decoded sentence: لن أسامحك\n","\n","-\n","Input sentence: i laughed\n","Decoded sentence: أنا أحسدها\n","\n","-\n","Input sentence: i like it\n","Decoded sentence: أنا أحبك\n","\n","-\n","Input sentence: i love it\n","Decoded sentence: أنا أحسدها\n","\n","-\n","Input sentence: i see tom\n","Decoded sentence: أنا أريك كذيرا\n","\n","-\n","Input sentence: im a boy\n","Decoded sentence: أنا ولد\n","\n","-\n","Input sentence: im a kid\n","Decoded sentence: أنا ولد\n","\n","-\n","Input sentence: im a man\n","Decoded sentence: أنا ولد\n","\n","-\n","Input sentence: im alone\n","Decoded sentence: أنا ولد\n","\n","-\n","Input sentence: im bored\n","Decoded sentence: أنا سعيد جدا\n","\n","-\n","Input sentence: im broke\n","Decoded sentence: أنا سعيد جدا\n","\n","-\n","Input sentence: im happy\n","Decoded sentence: أنا أ ريد تنا\n","\n","-\n","Input sentence: im lucky\n","Decoded sentence: أنا سعيد جدا\n","\n","-\n","Input sentence: im needy\n","Decoded sentence: أنا لست سعيدا\n","\n","-\n","Input sentence: im ready\n","Decoded sentence: أنا أ ريد تنا\n","\n","-\n","Input sentence: im ready\n","Decoded sentence: أنا أ ريد تنا\n","\n","-\n","Input sentence: im sorry\n","Decoded sentence: أنا سعيد للغاية\n","\n","-\n","Input sentence: im sorry\n","Decoded sentence: أنا سعيد للغاية\n","\n","-\n","Input sentence: im sorry\n","Decoded sentence: أنا سعيد للغاية\n","\n","-\n","Input sentence: im sorry\n","Decoded sentence: أنا سعيد للغاية\n","\n","-\n","Input sentence: im tired\n","Decoded sentence: أنا أ ريد تنا\n","\n","-\n","Input sentence: is it new\n","Decoded sentence: هل هذا لك\n","\n","-\n","Input sentence: its cold\n","Decoded sentence: إنه جديد\n","\n","-\n","Input sentence: its late\n","Decoded sentence: إنه جديد\n","\n","-\n","Input sentence: let me go\n","Decoded sentence: لنحتقن\n","\n","-\n","Input sentence: listen up\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: look back\n","Decoded sentence: انظر إلي\n","\n","-\n","Input sentence: of course\n","Decoded sentence: الحق قيد\n","\n","-\n","Input sentence: of course\n","Decoded sentence: الحق قيد\n","\n","-\n","Input sentence: of course\n","Decoded sentence: الحق قيد\n","\n","-\n","Input sentence: of course\n","Decoded sentence: الحق قيد\n","\n","-\n","Input sentence: of course\n","Decoded sentence: الحق قيد\n","\n","-\n","Input sentence: pardon me\n","Decoded sentence: إنها تلك ميضا\n","\n","-\n","Input sentence: seriously\n","Decoded sentence: هل أنت محق\n","\n","-\n","Input sentence: she walks\n","Decoded sentence: استدرت\n","\n","-\n","Input sentence: stay away\n","Decoded sentence: إنها تلسين\n","\n","-\n","Input sentence: stay thin\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: stay thin\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: stay thin\n","Decoded sentence: استمع\n","\n","-\n","Input sentence: stop that\n","Decoded sentence: توقفي عن الصراخ\n","\n","-\n","Input sentence: stop that\n","Decoded sentence: توقفي عن الصراخ\n","\n","-\n","Input sentence: stop them\n","Decoded sentence: توقفي عن الصراخ\n","\n","-\n","Input sentence: take care\n","Decoded sentence: اعتن بنفسك\n","\n","-\n","Input sentence: take care\n","Decoded sentence: اعتن بنفسك\n","\n","-\n","Input sentence: take care\n","Decoded sentence: اعتن بنفسك\n","\n","-\n","Input sentence: take care\n","Decoded sentence: اعتن بنفسك\n","\n","-\n","Input sentence: thank you\n","Decoded sentence: شكرا جزيلا\n","\n","-\n","Input sentence: then what\n","Decoded sentence: ما زالت الحشه\n","\n","-\n","Input sentence: then what\n","Decoded sentence: ما زالت الحشه\n","\n","-\n","Input sentence: tom moved\n","Decoded sentence: توم يحب التلفاز\n","\n","-\n","Input sentence: tom waved\n","Decoded sentence: توم يحبه\n","\n","-\n","Input sentence: watch out\n","Decoded sentence: انتظر دقيقة\n","\n","-\n","Input sentence: were hot\n","Decoded sentence: نحن سعداء\n","\n","-\n","Input sentence: whats up\n","Decoded sentence: ما الذي حصل البارحة\n","\n","-\n","Input sentence: who cares\n","Decoded sentence: من يدر سك الفرجة\n","\n","-\n","Input sentence: who is it\n","Decoded sentence: من يملك هذا المنزل\n","\n","-\n","Input sentence: who knows\n","Decoded sentence: من يملك هذا المنزل\n","\n","-\n","Input sentence: who knows\n","Decoded sentence: من يملك هذا المنزل\n","\n","-\n","Input sentence: wonderful\n","Decoded sentence: انتظر لحظة\n","\n"],"name":"stdout"}]}]}